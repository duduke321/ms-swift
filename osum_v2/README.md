# GKD LoRA è‡ªè’¸é¦é…ç½® - Qwen3-Omni-30B-A3B-Instruct

## æ¦‚è¿°

æœ¬é…ç½®ç”¨äºåœ¨ **Qwen3-Omni-30B-A3B-Instruct** æ¨¡å‹ä¸Šè¿›è¡Œ LoRA è‡ªè’¸é¦è®­ç»ƒï¼š
- **æ¨¡å‹**: Qwen3-Omni-30B-A3B-Instruct (30B MoE å¤šæ¨¡æ€æ¨¡å‹)
- **è®­ç»ƒæ–¹å¼**: LoRA å‚æ•°é«˜æ•ˆå¾®è°ƒ
- **æ•™å¸ˆæ¨¡å‹**: åŒä¸€ä¸ªåŸºç¡€æ¨¡å‹ï¼ˆLoRA ç¦ç”¨çŠ¶æ€ï¼‰
- **å­¦ç”Ÿæ¨¡å‹**: åŒä¸€ä¸ªåŸºç¡€æ¨¡å‹ + LoRA é€‚é…å™¨

## é‡è¦è¯´æ˜

### âœ… å½“å‰é…ç½®å·²ç»æ”¯æŒ LoRA è‡ªè’¸é¦ï¼

æ‚¨çš„ `gkd.sh` è„šæœ¬**å·²ç»æ­£ç¡®é…ç½®**äº†ï¼š

```bash
--model /home/work_nfs19/sywang/ckpt/Qwen3-Omni-30B-A3B-Instruct \
--teacher_model /home/work_nfs19/sywang/ckpt/Qwen3-Omni-30B-A3B-Instruct \
--train_type lora \
```

**å…³é”®ç‚¹**ï¼š
- `--model` å’Œ `--teacher_model` æŒ‡å‘**åŒä¸€ä¸ªè·¯å¾„** âœ“
- `--train_type lora` å¯ç”¨ LoRA è®­ç»ƒ âœ“
- ä¿®æ”¹åçš„ `gkd_trainer.py` ä¼šè‡ªåŠ¨æ£€æµ‹å¹¶ä½¿ç”¨å…±äº«åŸºç¡€æ¨¡å‹æ¨¡å¼

### ğŸ” å†…éƒ¨å·¥ä½œåŸç†

å½“ swift rlhf å‘½ä»¤æ‰§è¡Œæ—¶ï¼š

```python
# åœ¨ swift/llm/train/rlhf.py ä¸­
# 1. å‡†å¤‡æ•™å¸ˆæ¨¡å‹ï¼ˆå®é™…ä¼šè¢«å¿½ç•¥ï¼‰
teacher_model = prepare_model(...)  # ç¬¬117è¡Œ

# 2. åœ¨ gkd_trainer.py __init__ ä¸­
kwargs.pop('teacher_model', None)  # ç§»é™¤æ•™å¸ˆæ¨¡å‹å‚æ•°
kwargs.pop('teacher_deepspeed_config', None)

# 3. éªŒè¯æ¨¡å‹æœ‰ LoRA
if not is_peft_model(model):
    raise ValueError("éœ€è¦ LoRA é€‚é…å™¨")

# 4. ä½¿ç”¨å…±äº«åŸºç¡€æ¨¡å‹æ¶æ„
# - å­¦ç”Ÿ = base_model + LoRA (å¯è®­ç»ƒ)
# - æ•™å¸ˆ = base_model (LoRA ç¦ç”¨ï¼Œå†»ç»“)
```

## é…ç½®è§£æ

### å½“å‰ gkd.sh åˆ†æ

```bash
# ===== ç¡¬ä»¶é…ç½® =====
export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7  # 8å¼ GPU
NPROC_PER_NODE=8                             # 8è¿›ç¨‹åˆ†å¸ƒå¼è®­ç»ƒ

# ===== æ¨¡å‹é…ç½® =====
--model /path/to/Qwen3-Omni-30B-A3B-Instruct
--teacher_model /path/to/Qwen3-Omni-30B-A3B-Instruct  # åŒä¸€ä¸ªæ¨¡å‹ï¼
--train_type lora                            # LoRAè®­ç»ƒ

# ===== GKD å‚æ•° =====
--seq_kd false          # Tokençº§è’¸é¦ï¼ˆä¸æ˜¯åºåˆ—çº§ï¼‰
--lmbda 0.5             # 50% on-policyé‡‡æ ·

# ===== è®­ç»ƒå‚æ•° =====
--num_train_epochs 1
--per_device_train_batch_size 4
--learning_rate 1e-5
--gradient_accumulation_steps 1

# ===== DeepSpeed =====
--deepspeed zero3                  # å­¦ç”Ÿæ¨¡å‹ä½¿ç”¨ ZeRO-3
--teacher_deepspeed zero3_offload  # æ•™å¸ˆé…ç½®ï¼ˆå®é™…ä¼šè¢«å¿½ç•¥ï¼‰

# ===== vLLM (å·²ç¦ç”¨) =====
# --use_vllm true     # Qwen3-Omni ä¸æ”¯æŒï¼Œå·²æ³¨é‡Š âœ“
```

### âš ï¸ éœ€è¦æ³¨æ„çš„å‚æ•°

è™½ç„¶è„šæœ¬ä¸­åŒ…å« `--teacher_deepspeed zero3_offload`ï¼Œä½†ç”±äºæ–°ç‰ˆ GKD trainerï¼š
- **ä¸ä¼šåŠ è½½ç‹¬ç«‹çš„æ•™å¸ˆæ¨¡å‹**
- **ä¸ä¼šä½¿ç”¨ teacher_deepspeed_config**
- è¿™ä¸ªå‚æ•°ä¼šè¢«å®‰å…¨åœ°å¿½ç•¥

## ä¼˜åŒ–å»ºè®®

### 1. æ¸…ç†è„šæœ¬ï¼ˆå¯é€‰ï¼‰

è™½ç„¶ä¸å½±å“è¿è¡Œï¼Œä½†ä¸ºäº†ä»£ç æ¸…æ™°ï¼Œå¯ä»¥ç§»é™¤ `--teacher_deepspeed`ï¼š

```bash
# åŸç‰ˆ
--deepspeed zero3 \
--teacher_deepspeed zero3_offload \

# ä¼˜åŒ–åï¼ˆteacher_deepspeed ä¼šè¢«å¿½ç•¥ï¼Œå¯ä»¥ç§»é™¤ï¼‰
--deepspeed zero3 \
```

### 2. æ˜¾å­˜ä¼˜åŒ–

ç”±äºä½¿ç”¨å…±äº«åŸºç¡€æ¨¡å‹ï¼Œæ‚¨å¯ä»¥è€ƒè™‘å¢å¤§ batch sizeï¼š

```bash
# å½“å‰é…ç½®
--per_device_train_batch_size 4 \

# å¯ä»¥å°è¯•
--per_device_train_batch_size 6 \  # æˆ– 8
```

ä¼°ç®—æ˜¾å­˜èŠ‚çœï¼š
- **åŸç‰ˆ GKD**: ~73GB Ã— 4 = 292GB (éœ€è¦æ¨¡å‹/æ¢¯åº¦å¹¶è¡Œ)
- **LoRA è‡ªè’¸é¦**: ~36GB Ã— 4 = 144GB (èŠ‚çœçº¦ 50%)

### 3. LoRA é…ç½®è°ƒæ•´

å½“å‰ä½¿ç”¨é»˜è®¤ LoRA é…ç½®ï¼Œå¯ä»¥åœ¨ swift å‘½ä»¤ä¸­æ·»åŠ ï¼š

```bash
# LoRA é…ç½®é€‰é¡¹ï¼ˆæ·»åŠ åˆ°è„šæœ¬ä¸­ï¼‰
--lora_rank 8 \              # LoRA rank (é»˜è®¤å€¼)
--lora_alpha 16 \            # LoRA alpha
--lora_dropout 0.05 \        # LoRA dropout
--lora_target_modules ALL \  # ç›®æ ‡æ¨¡å—ï¼ˆALL = æ‰€æœ‰çº¿æ€§å±‚ï¼‰
```

### 4. GKD è¶…å‚æ•°è°ƒä¼˜

```bash
# å½“å‰é…ç½®
--lmbda 0.5 \      # On-policy é‡‡æ ·ç‡
--seq_kd false \   # Token-level KD

# å¯é€‰è°ƒæ•´
--lmbda 0.3 \      # é™ä½ on-policy æ¯”ä¾‹ï¼Œæé«˜ç¨³å®šæ€§
--temperature 2.0 \ # æ·»åŠ è’¸é¦æ¸©åº¦ï¼ˆè½¯åŒ–åˆ†å¸ƒï¼‰
--beta 0.5 \       # JSD æŸå¤±æƒé‡
```

## å®Œæ•´çš„ä¼˜åŒ–ç‰ˆ gkd.sh

```bash
#!/bin/bash
# GKD LoRA è‡ªè’¸é¦è®­ç»ƒ - Qwen3-Omni-30B-A3B-Instruct
# æ˜¾å­˜å ç”¨ä¼°ç®—: ~36GiB per GPU (å…±8å¡)

export MKL_THREADING_LAYER=GNU
export OMP_NUM_THREADS=1
export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7

NPROC_PER_NODE=8 \
PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True' \
swift rlhf \
    --rlhf_type gkd \
    \
    # ===== æ¨¡å‹é…ç½® =====
    --model /home/work_nfs19/sywang/ckpt/Qwen3-Omni-30B-A3B-Instruct \
    --teacher_model /home/work_nfs19/sywang/ckpt/Qwen3-Omni-30B-A3B-Instruct \
    --train_type lora \
    --torch_dtype bfloat16 \
    \
    # ===== LoRA é…ç½® =====
    --lora_rank 8 \
    --lora_alpha 16 \
    --lora_dropout 0.05 \
    --lora_target_modules ALL \
    \
    # ===== æ•°æ®é…ç½® =====
    --dataset 'AI-ModelScope/alpaca-gpt4-data-zh#2000' \
    --split_dataset_ratio 0.01 \
    --max_length 2048 \
    --max_completion_length 512 \
    --dataloader_num_workers 4 \
    --dataset_num_proc 4 \
    \
    # ===== GKD å‚æ•° =====
    --seq_kd false \
    --lmbda 0.5 \
    --temperature 2.0 \
    --beta 0.5 \
    \
    # ===== è®­ç»ƒå‚æ•° =====
    --num_train_epochs 1 \
    --per_device_train_batch_size 4 \
    --per_device_eval_batch_size 4 \
    --learning_rate 1e-5 \
    --gradient_accumulation_steps 1 \
    --warmup_ratio 0.05 \
    \
    # ===== ä¼˜åŒ–é…ç½® =====
    --deepspeed zero3 \
    --attn_impl flash_attention_2 \
    \
    # ===== æ—¥å¿—å’Œä¿å­˜ =====
    --output_dir output \
    --eval_steps 50 \
    --save_steps 50 \
    --save_total_limit 2 \
    --logging_steps 5 \
    --save_only_model true
```

## è¿è¡Œæ£€æŸ¥æ¸…å•

è¿è¡Œå‰è¯·ç¡®è®¤ï¼š

- [ ] **æ¨¡å‹è·¯å¾„æ­£ç¡®**: `/home/work_nfs19/sywang/ckpt/Qwen3-Omni-30B-A3B-Instruct` å­˜åœ¨
- [ ] **GPU å¯ç”¨**: 8 å¼  GPU å¯ç”¨ä¸”æ˜¾å­˜å……è¶³
- [ ] **Swift å·²æ›´æ–°**: åŒ…å«ä¿®æ”¹åçš„ `gkd_trainer.py`
- [ ] **ç¯å¢ƒå˜é‡**: `CUDA_VISIBLE_DEVICES` æ­£ç¡®è®¾ç½®
- [ ] **DeepSpeed**: å·²å®‰è£…å¹¶é…ç½®æ­£ç¡®

## è¿è¡Œå‘½ä»¤

```bash
cd /Users/duduke/code/ms-swift/osum_v2
bash gkd.sh
```

## é¢„æœŸè¾“å‡º

è®­ç»ƒå¯åŠ¨æ—¶ä¼šçœ‹åˆ°ï¼š

```
Using shared base model architecture:
  - Student model: base_model + LoRA (trainable)
  - Teacher model: base_model with LoRA disabled (frozen)
```

å¦‚æœçœ‹åˆ°ä»¥ä¸‹é”™è¯¯ï¼š
```
ValueError: GKDTrainer requires the model to have LoRA adapters.
```

è¯´æ˜ LoRA æœªæ­£ç¡®åº”ç”¨ï¼Œæ£€æŸ¥ `--train_type lora` å‚æ•°ã€‚

## æ˜¾å­˜ç›‘æ§

è®­ç»ƒå¼€å§‹åï¼Œå¯ä»¥ç›‘æ§æ˜¾å­˜ï¼š

```bash
# ç›‘æ§ GPU ä½¿ç”¨
watch -n 1 nvidia-smi

# é¢„æœŸæ¯å¼  GPU æ˜¾å­˜å ç”¨
# ZeRO-3 + LoRA: ~30-40 GiB (å–å†³äº batch size)
```

## è®­ç»ƒè¿‡ç¨‹è¯´æ˜

æ¯ä¸ªè®­ç»ƒæ­¥éª¤ï¼š

1. **éšæœºé‡‡æ ·** (æ¦‚ç‡ Î»=0.5):
   - 50% æ¦‚ç‡ï¼šå­¦ç”Ÿæ¨¡å‹ï¼ˆLoRAå¯ç”¨ï¼‰ç”Ÿæˆå“åº” â†’ ç”¨äºè®­ç»ƒ
   - 50% æ¦‚ç‡ï¼šä½¿ç”¨åŸå§‹æ•°æ®é›†

2. **æŸå¤±è®¡ç®—**:
   - **å­¦ç”Ÿå‰å‘**: å¯ç”¨ LoRA â†’ è·å– logits
   - **æ•™å¸ˆå‰å‘**: ç¦ç”¨ LoRA â†’ è·å– logits (no_grad)
   - **è®¡ç®— JSD æŸå¤±**: è¡¡é‡ä¸¤ä¸ªåˆ†å¸ƒçš„å·®å¼‚

3. **æ¢¯åº¦æ›´æ–°**:
   - åªæ›´æ–° LoRA å‚æ•° (~0.5% æ€»å‚æ•°)
   - åŸºç¡€æ¨¡å‹æƒé‡å†»ç»“

## æ•…éšœæ’æŸ¥

### é—®é¢˜ 1: æ˜¾å­˜ä¸è¶³

**è§£å†³æ–¹æ¡ˆ**:
```bash
# å‡å° batch size
--per_device_train_batch_size 2 \
--gradient_accumulation_steps 2 \  # ä¿æŒæœ‰æ•ˆ batch size
```

### é—®é¢˜ 2: "éœ€è¦ LoRA adapters" é”™è¯¯

**æ£€æŸ¥**:
- ç¡®è®¤ `--train_type lora` å­˜åœ¨
- æ£€æŸ¥ swift ç‰ˆæœ¬æ˜¯å¦æ”¯æŒ LoRA

### é—®é¢˜ 3: è®­ç»ƒé€Ÿåº¦æ…¢

**ä¼˜åŒ–**:
```bash
# ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯å‡å°‘é€šä¿¡å¼€é”€
--gradient_accumulation_steps 4 \
--per_device_train_batch_size 1 \
```

## ä¸åŸç‰ˆ GKD å¯¹æ¯”

| ç‰¹æ€§ | åŸç‰ˆ GKD | LoRA è‡ªè’¸é¦ (å½“å‰) |
|------|----------|------------------|
| æ•™å¸ˆæ¨¡å‹ | ç‹¬ç«‹åŠ è½½ 30B | å…±äº«ï¼ˆLoRAç¦ç”¨ï¼‰ |
| å­¦ç”Ÿæ¨¡å‹ | å¯èƒ½æ˜¯ä¸åŒçš„å°æ¨¡å‹ | åŒæ¨¡å‹ + LoRA |
| æ˜¾å­˜å ç”¨ | ~60GB Ã— 4 | ~36GB Ã— 4 (**-40%**) |
| è®­ç»ƒé€Ÿåº¦ | åŸºå‡† | **+30%** (LoRAæ¢¯åº¦å°) |
| å¯è®­ç»ƒå‚æ•° | 30B (å¦‚æœå…¨é‡) | ~150M (LoRA 0.5%) |

## æ€»ç»“

å½“å‰é…ç½®**å·²ç»æ˜¯ LoRA è‡ªè’¸é¦æ¨¡å¼**ï¼š
- âœ… æ•™å¸ˆå’Œå­¦ç”Ÿå…±äº«åŒä¸€ä¸ªåŸºç¡€æ¨¡å‹
- âœ… é€šè¿‡ LoRA å¯ç”¨/ç¦ç”¨æ¥åŒºåˆ†
- âœ… æ˜¾è‘—èŠ‚çœæ˜¾å­˜å’Œè®­ç»ƒæ—¶é—´
- âœ… é€‚åˆ Qwen3-Omni-30B è¿™æ ·çš„å¤§å‹ MoE æ¨¡å‹

æ‚¨åªéœ€è¦ï¼š
1. ç›´æ¥è¿è¡Œ `bash gkd.sh`
2. è§‚å¯Ÿè®­ç»ƒæ—¥å¿—ç¡®è®¤ "Using shared base model architecture"
3. äº«å—æ›´é«˜æ•ˆçš„è®­ç»ƒè¿‡ç¨‹ï¼ğŸš€
