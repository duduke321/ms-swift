# GKD LoRA è‡ªè’¸é¦å®Œæ•´æŒ‡å—
## Qwen3-Omni-30B-A3B-Instruct è®­ç»ƒæ–‡æ¡£

> æœ¬æ–‡æ¡£æ•´åˆäº†æ‰€æœ‰å…³äº GKD LoRA è‡ªè’¸é¦è®­ç»ƒçš„å†…å®¹ï¼ŒåŒ…æ‹¬æ¦‚å¿µã€åŸç†ã€é…ç½®ã€ä¼˜åŒ–å’Œæ•…éšœæ’æŸ¥ã€‚é˜…è¯»æœ¬æ–‡æ¡£å³å¯å…¨é¢äº†è§£æ•´ä¸ªè®­ç»ƒæµç¨‹ã€‚

---

## ğŸ“‹ ç›®å½•

1. [æ ¸å¿ƒæ¦‚å¿µ](#1-æ ¸å¿ƒæ¦‚å¿µ)
2. [å¿«é€Ÿå¼€å§‹](#2-å¿«é€Ÿå¼€å§‹)
3. [é…ç½®è¯¦è§£](#3-é…ç½®è¯¦è§£)
4. [è®­ç»ƒæµç¨‹è¯¦è§£](#4-è®­ç»ƒæµç¨‹è¯¦è§£)
5. [æŸå¤±è®¡ç®—æœºåˆ¶](#5-æŸå¤±è®¡ç®—æœºåˆ¶)
6. [å‚æ•°è°ƒä¼˜](#6-å‚æ•°è°ƒä¼˜)
7. [æ˜¾å­˜ç®¡ç†](#7-æ˜¾å­˜ç®¡ç†)
8. [æ•…éšœæ’æŸ¥](#8-æ•…éšœæ’æŸ¥)
9. [é«˜çº§æŠ€å·§](#9-é«˜çº§æŠ€å·§)
10. [æŠ€æœ¯æ·±å…¥](#10-æŠ€æœ¯æ·±å…¥)

---

## 1. æ ¸å¿ƒæ¦‚å¿µ

### 1.1 ä»€ä¹ˆæ˜¯ GKD LoRA è‡ªè’¸é¦ï¼Ÿ

**ä¼ ç»ŸçŸ¥è¯†è’¸é¦**ï¼š
```
æ•™å¸ˆæ¨¡å‹ (å¤§æ¨¡å‹ 30B) â†’ è¾“å‡ºåˆ†å¸ƒ A
                          â†“ è’¸é¦æŸå¤±
å­¦ç”Ÿæ¨¡å‹ (å°æ¨¡å‹ 7B)  â†’ è¾“å‡ºåˆ†å¸ƒ B
```

**LoRA è‡ªè’¸é¦ï¼ˆæœ¬å®ç°ï¼‰**ï¼š
```
        åŒä¸€ä¸ªåŸºç¡€æ¨¡å‹ (Qwen3-Omni-30B)
                    â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚                 â”‚
      LoRA ç¦ç”¨          LoRA å¯ç”¨
      (æ•™å¸ˆæ¨¡å¼)         (å­¦ç”Ÿæ¨¡å¼)
           â”‚                 â”‚
      å†»ç»“é¢„æµ‹          å¯è®­ç»ƒé¢„æµ‹
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
                JSD æŸå¤±
                    â†“
          åªæ›´æ–° LoRA å‚æ•°
```

### 1.2 æ ¸å¿ƒä¼˜åŠ¿

âœ… **å…±äº«åŸºç¡€æƒé‡** - èŠ‚çœ 40-50% æ˜¾å­˜  
âœ… **æ¨¡å‹è‡ªæˆ‘æ”¹è¿›** - å­¦ä¹ å¦‚ä½•é€šè¿‡ LoRA å¢å¼ºè‡ªå·±  
âœ… **è®­ç»ƒé«˜æ•ˆ** - LoRA å‚æ•°é‡å°ï¼ˆ~0.5%ï¼‰ï¼Œæ¢¯åº¦è®¡ç®—å¿«  
âœ… **æ— éœ€é¢å¤–æ•™å¸ˆ** - åŸºç¡€æ¨¡å‹è‡ªå·±å°±æ˜¯æ•™å¸ˆ  
âœ… **é€‚åˆå¤§æ¨¡å‹** - Qwen3-Omni-30B è¿™æ ·çš„ MoE æ¨¡å‹

### 1.3 ä¸€å›¾çœ‹æ‡‚è®­ç»ƒæµç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ä¸€ä¸ªè®­ç»ƒæ­¥éª¤                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Step 1: å†³å®šè®­ç»ƒæ¨¡å¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ random() ç”Ÿæˆâ”‚
â”‚ éšæœºæ•° r     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
   â”Œâ”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚                    â”‚
r â‰¤ 0.5              r > 0.5
   â†“                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚On-Policy â”‚      â”‚Off-Policyâ”‚
â”‚å­¦ç”Ÿç”Ÿæˆ  â”‚      â”‚ä½¿ç”¨åŸå§‹  â”‚
â”‚æ–°æ•°æ®    â”‚      â”‚æ•°æ®é›†    â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
      â”‚                  â”‚
      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“

Step 2: åŒå‰å‘ä¼ æ’­
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   å…±äº«åŸºç¡€æ¨¡å‹ (30Bå‚æ•°)      â”‚
â”‚                              â”‚
â”‚    åŸºç¡€ Transformer å±‚        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
    â”‚             â”‚
LoRA ç¦ç”¨     LoRA å¯ç”¨
(æ•™å¸ˆ)        (å­¦ç”Ÿ)
    â”‚             â”‚
    â†“             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚åŸºç¡€è¾“å‡ºâ”‚  â”‚åŸºç¡€+LoRAâ”‚
â”‚ Logits â”‚  â”‚ Logits â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
    â”‚           â”‚
    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
          â†“

Step 3: è®¡ç®— JSD æŸå¤±
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ P_teacher    P_student     â”‚
â”‚     â”‚           â”‚          â”‚
â”‚     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜          â”‚
â”‚           â†“                â”‚
â”‚    M = 0.5*P_t + 0.5*P_s  â”‚
â”‚     (æ··åˆåˆ†å¸ƒ)             â”‚
â”‚           â”‚                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â†“                 â†“       â”‚
â”‚ KL(P_t||M)    KL(P_s||M)  â”‚
â”‚  â”‚                 â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚           â†“                â”‚
â”‚    JSD = 0.5*KL_t + 0.5*KL_sâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“

Step 4: åå‘ä¼ æ’­ï¼ˆåªæ›´æ–° LoRAï¼‰
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Loss.backward()         â”‚
â”‚            â†“                 â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚   â”‚ åŸºç¡€å‚æ•° (å†»ç»“) â”‚        â”‚
â”‚   â”‚ æ¢¯åº¦ = None     â”‚        â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚            â†“                 â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚   â”‚ LoRA å‚æ•° âœ“     â”‚        â”‚
â”‚   â”‚ æ¢¯åº¦å·²è®¡ç®—      â”‚        â”‚
â”‚   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚        â†“                     â”‚
â”‚   Optimizer.step()           â”‚
â”‚   W_lora -= lr * grad        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. å¿«é€Ÿå¼€å§‹

### 2.1 âœ… å½“å‰é…ç½®éªŒè¯

æ‚¨çš„ `gkd.sh` **å·²ç»æ­£ç¡®é…ç½®**äº† LoRA è‡ªè’¸é¦ï¼

å…³é”®é…ç½®ï¼š
```bash
--model /home/work_nfs19/sywang/ckpt/Qwen3-Omni-30B-A3B-Instruct \
--teacher_model /home/work_nfs19/sywang/ckpt/Qwen3-Omni-30B-A3B-Instruct \  # åŒä¸€ä¸ªè·¯å¾„ï¼
--train_type lora \  # å¯ç”¨ LoRA
```

**ä¸ºä»€ä¹ˆè¿™æ ·é…ç½®å°±å¯ä»¥äº†ï¼Ÿ**
- `--model` å’Œ `--teacher_model` æŒ‡å‘åŒä¸€è·¯å¾„ â†’ å…±äº«åŸºç¡€æ¨¡å‹ âœ“
- `--train_type lora` â†’ å¯ç”¨ LoRA é€‚é…å™¨ âœ“
- ä¿®æ”¹åçš„ `gkd_trainer.py` ä¼šè‡ªåŠ¨æ£€æµ‹å¹¶ä½¿ç”¨å…±äº«æ¨¡å¼

### 2.2 ç«‹å³è¿è¡Œ

```bash
# æ–¹æ³•1: è¿è¡ŒåŸå§‹è„šæœ¬
cd /Users/duduke/code/ms-swift/osum_v2
bash gkd.sh

# æ–¹æ³•2: è¿è¡Œä¼˜åŒ–ç‰ˆè„šæœ¬
bash gkd_optimized.sh
```

### 2.3 è¿è¡Œæ£€æŸ¥æ¸…å•

è®­ç»ƒå‰ç¡®è®¤ï¼š

- [ ] **æ¨¡å‹è·¯å¾„å­˜åœ¨**: `/home/work_nfs19/sywang/ckpt/Qwen3-Omni-30B-A3B-Instruct`
- [ ] **8å¼  GPU å¯ç”¨**: `nvidia-smi` æ˜¾ç¤º 8 å¼ æ˜¾å¡
- [ ] **æ˜¾å­˜å……è¶³**: æ¯å¼ å¡è‡³å°‘ 40GB å¯ç”¨
- [ ] **Swift å·²æ›´æ–°**: åŒ…å«ä¿®æ”¹åçš„ `gkd_trainer.py`
- [ ] **DeepSpeed å·²å®‰è£…**: `pip show deepspeed`

### 2.4 é¢„æœŸè¾“å‡º

è®­ç»ƒå¯åŠ¨æ—¶ä¼šçœ‹åˆ°ï¼š

```
âœ“ Using shared base model architecture:
  - Student model: base_model + LoRA (trainable)
  - Teacher model: base_model with LoRA disabled (frozen)

trainable params: 150,994,944 (0.50% of 30,573,527,040)
```

å¦‚æœçœ‹åˆ°ä»¥ä¸‹é”™è¯¯ï¼š
```
âœ— ValueError: GKDTrainer requires the model to have LoRA adapters.
```
è¯´æ˜ LoRA æœªæ­£ç¡®åº”ç”¨ï¼Œæ£€æŸ¥ `--train_type lora` å‚æ•°ã€‚

---

## 3. é…ç½®è¯¦è§£

### 3.1 å½“å‰ gkd.sh é…ç½®åˆ†æ

```bash
#!/bin/bash

# ===== ç¯å¢ƒå˜é‡ =====
export MKL_THREADING_LAYER=GNU     # Intel MKL çº¿ç¨‹å±‚
export OMP_NUM_THREADS=1           # OpenMP çº¿ç¨‹æ•°
export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7  # 8å¼ GPU

# ===== åˆ†å¸ƒå¼é…ç½® =====
NPROC_PER_NODE=8                   # 8ä¸ªè¿›ç¨‹ï¼ˆæ¯å¼ å¡ä¸€ä¸ªï¼‰
PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True'  # æ˜¾å­˜ä¼˜åŒ–

# ===== åŸºç¡€é…ç½® =====
swift rlhf \
    --rlhf_type gkd \              # ä½¿ç”¨ GKD è®­ç»ƒå™¨
    \
    # ===== æ¨¡å‹é…ç½® =====
    --model /path/to/Qwen3-Omni-30B-A3B-Instruct \
    --teacher_model /path/to/Qwen3-Omni-30B-A3B-Instruct \  # åŒä¸€ä¸ªï¼
    --train_type lora \            # LoRA è®­ç»ƒ
    --torch_dtype bfloat16 \       # BF16 ç²¾åº¦
    \
    # ===== æ•°æ®é…ç½® =====
    --dataset 'AI-ModelScope/alpaca-gpt4-data-zh#2000' \
    --split_dataset_ratio 0.01 \   # 1% ç”¨äºéªŒè¯
    --max_length 2048 \             # æœ€å¤§åºåˆ—é•¿åº¦
    --max_completion_length 512 \  # æœ€å¤§ç”Ÿæˆé•¿åº¦
    --dataloader_num_workers 4 \   # æ•°æ®åŠ è½½å¹¶è¡Œåº¦
    --dataset_num_proc 4 \         # æ•°æ®é¢„å¤„ç†å¹¶è¡Œåº¦
    \
    # ===== GKD å‚æ•° =====
    --seq_kd false \               # Tokençº§è’¸é¦ï¼ˆä¸æ˜¯åºåˆ—çº§ï¼‰
    --lmbda 0.5 \                  # 50% on-policyé‡‡æ ·
    \
    # ===== è®­ç»ƒå‚æ•° =====
    --num_train_epochs 1 \
    --per_device_train_batch_size 4 \
    --per_device_eval_batch_size 4 \
    --learning_rate 1e-5 \
    --gradient_accumulation_steps 1 \
    --warmup_ratio 0.05 \          # 5% warmup
    \
    # ===== ä¼˜åŒ–é…ç½® =====
    --deepspeed zero3 \            # ZeRO-3 ä¼˜åŒ–
    --attn_impl flash_attention_2 \  # Flash Attention
    \
    # ===== æ—¥å¿—å’Œä¿å­˜ =====
    --output_dir output \
    --eval_steps 50 \
    --save_steps 50 \
    --save_total_limit 2 \         # åªä¿ç•™æœ€è¿‘2ä¸ªæ£€æŸ¥ç‚¹
    --logging_steps 5 \
    --save_only_model true
```

### 3.2 å‚æ•°é€ŸæŸ¥è¡¨

#### GKD å‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | å»ºè®®èŒƒå›´ | è¯´æ˜ |
|------|--------|----------|------|
| `lmbda` | 0.5 | 0.3-0.7 | On-policy é‡‡æ ·æ¦‚ç‡ã€‚è¶Šé«˜è¶Šå¤šæ¢ç´¢ |
| `temperature` | 2.0 | 1.0-4.0 | è’¸é¦æ¸©åº¦ã€‚è¶Šé«˜åˆ†å¸ƒè¶Šå¹³æ»‘ |
| `beta` | 0.5 | 0.3-0.7 | JSD æŸå¤±æƒé‡ï¼ˆå­¦ç”Ÿvsæ•™å¸ˆï¼‰ |
| `seq_kd` | false | true/false | åºåˆ—çº§ vs Tokençº§è’¸é¦ |
| `sft_alpha` | 0.0 | 0.0-0.5 | SFTæŸå¤±æƒé‡ï¼ˆå¯é€‰ï¼‰ |

#### LoRA å‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | å»ºè®®èŒƒå›´ | è¯´æ˜ |
|------|--------|----------|------|
| `lora_rank` | 8 | 4-64 | LoRA ç§©ã€‚è¶Šå¤§å®¹é‡è¶Šå¤§ |
| `lora_alpha` | 16 | 8-128 | LoRA alphaï¼Œé€šå¸¸=2Ã—rank |
| `lora_dropout` | 0.05 | 0.0-0.1 | Dropout ç‡ |
| `lora_target_modules` | ALL | ALL/specific | åº”ç”¨ LoRA çš„æ¨¡å— |

#### è®­ç»ƒå‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | å»ºè®®èŒƒå›´ | è¯´æ˜ |
|------|--------|----------|------|
| `per_device_train_batch_size` | 4 | 1-8 | æ¯å¡ batch size |
| `gradient_accumulation_steps` | 1 | 1-8 | æ¢¯åº¦ç´¯ç§¯æ­¥æ•° |
| `learning_rate` | 1e-5 | 5e-6 ~ 5e-5 | å­¦ä¹ ç‡ |
| `warmup_ratio` | 0.05 | 0.03-0.1 | Warmup æ¯”ä¾‹ |
| `max_length` | 2048 | 512-4096 | æœ€å¤§åºåˆ—é•¿åº¦ |

### 3.3 âš ï¸ é‡è¦è¯´æ˜

è™½ç„¶è„šæœ¬ä¸­åŒ…å« `--teacher_deepspeed zero3_offload`ï¼Œä½†å› ä¸ºæ–°ç‰ˆ GKD trainerï¼š
- **ä¸ä¼šåŠ è½½ç‹¬ç«‹çš„æ•™å¸ˆæ¨¡å‹**
- **ä¸ä¼šä½¿ç”¨ teacher_deepspeed_config**
- è¿™ä¸ªå‚æ•°ä¼šè¢«å®‰å…¨åœ°å¿½ç•¥ï¼ˆå¯ä»¥åˆ é™¤ä¿æŒæ¸…æ´ï¼‰

---

## 4. è®­ç»ƒæµç¨‹è¯¦è§£

### 4.1 å®Œæ•´è®­ç»ƒå¾ªç¯

```python
for epoch in range(num_epochs):
    for batch in dataloader:
        # ===== æ­¥éª¤ 1: å†³å®šè®­ç»ƒæ¨¡å¼ =====
        random_num = random()
        
        if random_num <= lmbda:  # ä¾‹å¦‚ lmbda=0.5ï¼Œ50%æ¦‚ç‡
            # On-Policy æ¨¡å¼ï¼šä½¿ç”¨å­¦ç”Ÿæ¨¡å‹ç”Ÿæˆæ•°æ®
            mode = "on_policy"
            inputs = generate_with_student(batch)
        else:
            # Off-Policy æ¨¡å¼ï¼šä½¿ç”¨åŸå§‹æ•°æ®
            mode = "off_policy"
            inputs = batch
        
        # ===== æ­¥éª¤ 2: è®¡ç®—æŸå¤± =====
        loss = compute_loss(model, inputs)
        
        # ===== æ­¥éª¤ 3: åå‘ä¼ æ’­å’Œä¼˜åŒ– =====
        loss.backward()
        optimizer.step()
```

### 4.2 On-Policy vs Off-Policy

#### On-Policyï¼ˆè‡ªæˆ‘ç”Ÿæˆï¼‰

å½“ `random() <= lmbda` æ—¶ï¼š

```python
# 1. ä½¿ç”¨å­¦ç”Ÿæ¨¡å‹ï¼ˆLoRAå¯ç”¨ï¼‰ç”Ÿæˆæ–°å“åº”
with unwrap_model_for_generation(model) as unwrapped_model:
    unwrapped_model.eval()
    new_sequences = unwrapped_model.generate(
        input_ids=batch['input_ids'],
        max_new_tokens=512,
        temperature=1.0,
        do_sample=True
    )
    unwrapped_model.train()

# 2. ç”¨ç”Ÿæˆçš„æ–°æ•°æ®æ›¿æ¢åŸå§‹å“åº”
inputs['input_ids'] = new_sequences
inputs['labels'] = create_labels(new_sequences)  # å¿½ç•¥ prompt éƒ¨åˆ†
```

**ä½œç”¨**ï¼š
- æ¢ç´¢å­¦ç”Ÿæ¨¡å‹å½“å‰çš„ç”Ÿæˆèƒ½åŠ›
- é¿å…åˆ†å¸ƒåç§»ï¼ˆå­¦ç”Ÿåªå­¦ä¹ æ•°æ®é›†åˆ†å¸ƒï¼‰
- æŒç»­æ”¹è¿›ç”Ÿæˆè´¨é‡

#### Off-Policyï¼ˆä½¿ç”¨æ•°æ®é›†ï¼‰

å½“ `random() > lmbda` æ—¶ï¼š

```python
# ç›´æ¥ä½¿ç”¨æ•°æ®é›†ä¸­çš„åŸå§‹æ•°æ®
inputs = batch
# åŒ…å« prompt å’Œ ground truth response
```

**ä½œç”¨**ï¼š
- å­¦ä¹ æ•°æ®é›†çš„æ ‡å‡†æ¨¡å¼
- ä¿è¯åŸºç¡€è´¨é‡
- æä¾›ç¨³å®šçš„è®­ç»ƒä¿¡å·

### 4.3 å®Œæ•´è®­ç»ƒç¤ºä¾‹

#### Epoch 1, Batch 1

```
â”Œâ”€ è¾“å…¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Prompt: "ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½"     â”‚
â”‚ Response: "äººå·¥æ™ºèƒ½æ˜¯..."     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€ Step 1: é‡‡æ · â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ random() = 0.31 < 0.5 (lmbda) â”‚
â”‚ â†’ On-Policy æ¨¡å¼              â”‚
â”‚ â†’ ä½¿ç”¨å­¦ç”Ÿæ¨¡å‹ç”Ÿæˆæ–°å“åº”       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€ ç”Ÿæˆé˜¶æ®µ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ å­¦ç”Ÿæ¨¡å‹ (LoRA å¯ç”¨):        â”‚
â”‚   Input: "ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½"   â”‚
â”‚   Generate: "äººå·¥æ™ºèƒ½æ˜¯ä¸€ç§..." â”‚ â† æ–°ç”Ÿæˆ
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€ Step 2: å‰å‘ä¼ æ’­ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æ•™å¸ˆ: Logits_t [no LoRA]     â”‚
â”‚ å­¦ç”Ÿ: Logits_s [+ LoRA]      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€ Step 3: è®¡ç®—æŸå¤± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ JSD(P_teacher, P_student)    â”‚
â”‚ = 0.234                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€ Step 4: åå‘ä¼ æ’­ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ loss.backward()              â”‚
â”‚ æ›´æ–° LoRA å‚æ•° (~150M)       â”‚
â”‚ åŸºç¡€å‚æ•°å†»ç»“ (~30B)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Epoch 1, Batch 2

```
â”Œâ”€ è¾“å…¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Prompt: "æ·±åº¦å­¦ä¹ æ˜¯ä»€ä¹ˆ"       â”‚
â”‚ Response: "æ·±åº¦å­¦ä¹ æ˜¯..."     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€ Step 1: é‡‡æ · â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ random() = 0.67 > 0.5 (lmbda) â”‚
â”‚ â†’ Off-Policy æ¨¡å¼             â”‚
â”‚ â†’ ä½¿ç”¨åŸå§‹æ•°æ®é›†å“åº”           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€ Step 2: å‰å‘ä¼ æ’­ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æ•™å¸ˆ: Logits_t [no LoRA]     â”‚
â”‚ å­¦ç”Ÿ: Logits_s [+ LoRA]      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€ Step 3: è®¡ç®—æŸå¤± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ JSD(P_teacher, P_student)    â”‚
â”‚ = 0.198                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€ Step 4: åå‘ä¼ æ’­ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ loss.backward()              â”‚
â”‚ æ›´æ–° LoRA å‚æ•°               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 5. æŸå¤±è®¡ç®—æœºåˆ¶

### 5.1 æ ¸å¿ƒï¼šJensen-Shannon Divergence (JSD)

JSD æ˜¯ä¸€ç§å¯¹ç§°çš„åˆ†å¸ƒè·ç¦»åº¦é‡ï¼Œç”¨äºè¡¡é‡æ•™å¸ˆå’Œå­¦ç”Ÿè¾“å‡ºåˆ†å¸ƒçš„å·®å¼‚ã€‚

**ä¸ºä»€ä¹ˆç”¨ JSD è€Œä¸æ˜¯ KLï¼Ÿ**

| ç‰¹æ€§ | KL æ•£åº¦ | JS æ•£åº¦ (JSD) |
|------|---------|---------------|
| å¯¹ç§°æ€§ | å¦ KL(Pâ€–Q) â‰  KL(Qâ€–P) | æ˜¯ JSD(Pâ€–Q) = JSD(Qâ€–P) |
| æœ‰ç•Œæ€§ | æ— ç•Œ [0, âˆ) | æœ‰ç•Œ [0, 1] |
| æ•°å€¼ç¨³å®šæ€§ | å·®ï¼ˆQ=0æ—¶çˆ†ç‚¸ï¼‰ | å¥½ï¼ˆé€šè¿‡æ··åˆåˆ†å¸ƒé¿å…ï¼‰ |
| æ¢¯åº¦ | å¯èƒ½ä¸ç¨³å®š | å¹³æ»‘ç¨³å®š |

### 5.2 å®Œæ•´è®¡ç®—æµç¨‹

#### æ­¥éª¤ 1: å­¦ç”Ÿå‰å‘ä¼ æ’­ï¼ˆLoRA å¯ç”¨ï¼‰

```python
# å­¦ç”Ÿæ¨¡å‹ï¼šåŸºç¡€æ¨¡å‹ + LoRA
# LoRA å¤„äºå¯ç”¨çŠ¶æ€
outputs_student = model(**model_inputs)
# outputs_student.logits: [batch_size, seq_len, vocab_size]
# ä¾‹å¦‚: [4, 2048, 151936]ï¼ˆQwen3 è¯è¡¨å¤§å°ï¼‰
```

**æ¨¡å‹è®¡ç®—è·¯å¾„**ï¼š
```
Input [4, 2048]
  â†“
Embedding [4, 2048, 4096]
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Transformer Layer 1 â”‚
â”‚  - Self-Attn (åŸºç¡€) â”‚
â”‚    + LoRA (delta) âœ“â”‚ â† LoRA å¢é‡
â”‚  - MLP (åŸºç¡€)       â”‚
â”‚    + LoRA (delta) âœ“â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“ (é‡å¤ 60 å±‚)
LM Head [4, 2048, 151936]
  â†“
Logits (å­¦ç”Ÿ)
```

#### æ­¥éª¤ 2: æ•™å¸ˆå‰å‘ä¼ æ’­ï¼ˆLoRA ç¦ç”¨ï¼‰

```python
# å…³é”®ï¼šç¦ç”¨ LoRA é€‚é…å™¨
with torch.no_grad(), self.disable_lora_context():
    outputs_teacher = model(**model_inputs)
    # outputs_teacher.logits: [batch_size, seq_len, vocab_size]
```

**disable_lora_context() å®ç°**ï¼š
```python
@contextmanager
def disable_lora_context(self):
    """ä¸´æ—¶ç¦ç”¨ LoRA é€‚é…å™¨"""
    unwrapped_model = self.accelerator.unwrap_model(self.model)
    
    if is_peft_model(unwrapped_model):
        # ç¦ç”¨æ‰€æœ‰ LoRA å±‚
        unwrapped_model.disable_adapter_layers()
        try:
            yield unwrapped_model
        finally:
            # é€€å‡ºæ—¶é‡æ–°å¯ç”¨
            unwrapped_model.enable_adapter_layers()
```

**æ¨¡å‹è®¡ç®—è·¯å¾„**ï¼ˆLoRA è¢«è·³è¿‡ï¼‰ï¼š
```
Input [4, 2048]
  â†“
Embedding [4, 2048, 4096]
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Transformer Layer 1 â”‚
â”‚  - Self-Attn (åŸºç¡€) â”‚ â† LoRA è¢«è·³è¿‡ âœ—
â”‚  - MLP (åŸºç¡€)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“ (é‡å¤ 60 å±‚)
LM Head [4, 2048, 151936]
  â†“
Logits (æ•™å¸ˆ)
```

#### æ­¥éª¤ 3: æå–æœ‰æ•ˆ Logits

```python
# åªå…³å¿ƒç”Ÿæˆéƒ¨åˆ†çš„ tokenï¼ˆä¸åŒ…æ‹¬ promptï¼‰
shifted_labels = torch.roll(inputs['labels'], shifts=-1, dims=1)
# å°† labels å‘å·¦ç§»åŠ¨ä¸€ä½ï¼Œä¸ logits å¯¹é½
# å› ä¸º logits[i] é¢„æµ‹çš„æ˜¯ token[i+1]

mask = shifted_labels != -100  # -100 è¡¨ç¤ºå¿½ç•¥çš„ä½ç½®ï¼ˆpromptéƒ¨åˆ†ï¼‰
# mask: [batch_size, seq_len] å¸ƒå°”å€¼

shifted_student_logits = outputs_student.logits[mask][None]
shifted_teacher_logits = outputs_teacher.logits[mask][None]
# å½¢çŠ¶: [1, num_valid_tokens, vocab_size]
# ä¾‹å¦‚: [1, 512, 151936]ï¼ˆåªä¿ç•™ 512 ä¸ªæœ‰æ•ˆç”Ÿæˆ tokenï¼‰
```

**ä¸ºä»€ä¹ˆè¿™æ ·å¤„ç†ï¼Ÿ**
- æˆ‘ä»¬åªæƒ³è’¸é¦**ç”Ÿæˆéƒ¨åˆ†**çš„çŸ¥è¯†ï¼Œä¸åŒ…æ‹¬è¾“å…¥ prompt
- Labels ä¸­ -100 æ ‡è®°çš„æ˜¯ prompt ä½ç½®

#### æ­¥éª¤ 4: è®¡ç®— JSD æŸå¤±

```python
def generalized_jsd_loss(student_logits, teacher_logits, beta=0.5, temperature=2.0):
    """
    Jensen-Shannon Divergence Loss
    
    å‚æ•°:
        student_logits: [1, N, V] å­¦ç”Ÿæ¨¡å‹ logits
        teacher_logits: [1, N, V] æ•™å¸ˆæ¨¡å‹ logits
        beta: æ··åˆç³»æ•° (0.5 è¡¨ç¤ºå‡åŒ€æ··åˆ)
        temperature: æ¸©åº¦å‚æ•°
    
    å…¬å¼:
        M = beta * P_student + (1-beta) * P_teacher  # æ··åˆåˆ†å¸ƒ
        JSD = beta * KL(P_student || M) + (1-beta) * KL(P_teacher || M)
    """
    
    # 1. åº”ç”¨æ¸©åº¦è½¯åŒ–
    student_logits = student_logits / temperature  # temperature=2.0
    teacher_logits = teacher_logits / temperature
    
    # 2. è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒ
    P_student = F.softmax(student_logits, dim=-1)  # [1, N, V]
    P_teacher = F.softmax(teacher_logits, dim=-1)
    
    # 3. è®¡ç®—æ··åˆåˆ†å¸ƒ
    M = beta * P_student + (1 - beta) * P_teacher
    
    # 4. è®¡ç®— KL æ•£åº¦
    kl_student = F.kl_div(
        F.log_softmax(student_logits, dim=-1),
        M,
        reduction='batchmean'
    )
    kl_teacher = F.kl_div(
        F.log_softmax(teacher_logits, dim=-1),
        M,
        reduction='batchmean'
    )
    
    # 5. ç»„åˆå¾—åˆ° JSD
    jsd_loss = beta * kl_student + (1 - beta) * kl_teacher
    
    return jsd_loss
```

### 5.3 æ¸©åº¦è½¯åŒ–çš„ä½œç”¨

**æ¸©åº¦è½¯åŒ–å‰**ï¼š
```
logits:      [10.2, 8.5, 1.3, 0.8, ...]
softmax:     [0.95, 0.04, 0.01, 0.00, ...]  â† åˆ†å¸ƒå¾ˆå°–é”
```

**æ¸©åº¦è½¯åŒ–å (T=2.0)**ï¼š
```
logits/T:    [5.1, 4.25, 0.65, 0.4, ...]
softmax:     [0.65, 0.28, 0.04, 0.03, ...]  â† åˆ†å¸ƒæ›´å¹³æ»‘
```

**ä½œç”¨**ï¼š
- è½¯åŒ–åˆ†å¸ƒï¼Œè®©å­¦ç”Ÿæ›´å®¹æ˜“å­¦ä¹ åˆ°"æš—çŸ¥è¯†"ï¼ˆdark knowledgeï¼‰
- å³ä½¿é”™è¯¯é€‰é¡¹çš„ç›¸å¯¹æ¦‚ç‡å…³ç³»ä¹Ÿå¾ˆé‡è¦
- T è¶Šå¤§ï¼Œåˆ†å¸ƒè¶Šå¹³æ»‘ï¼Œå®¹æ˜“å­¦ä¹ ï¼›T è¶Šå°ï¼Œè¶Šæ¥è¿‘ç¡¬æ ‡ç­¾

### 5.4 å¯é€‰çš„ SFT æŸå¤±

```python
if self.args.sft_alpha > 0:
    # æ·»åŠ ç›‘ç£å¾®è°ƒæŸå¤±ï¼ˆæ ‡å‡†äº¤å‰ç†µï¼‰
    sft_loss = F.cross_entropy(
        student_logits.view(-1, vocab_size),
        labels.view(-1),
        ignore_index=-100
    )
    loss = (1 - sft_alpha) * jsd_loss + sft_alpha * sft_loss
    # ä¾‹å¦‚: loss = 0.8 * jsd_loss + 0.2 * sft_loss
```

**æ··åˆæŸå¤±çš„ä½œç”¨**ï¼š
- **JSD Loss**: å­¦ä¹ æ•™å¸ˆçš„åˆ†å¸ƒæ¨¡å¼ï¼ˆè½¯æ ‡ç­¾ï¼‰â†’ æ³›åŒ–èƒ½åŠ›
- **SFT Loss**: å­¦ä¹ æ­£ç¡®çš„è¾“å‡ºï¼ˆç¡¬æ ‡ç­¾ï¼‰â†’ å‡†ç¡®æ€§
- ç»“åˆä¸¤è€…ï¼Œæ—¢ä¿è¯å‡†ç¡®æ€§åˆå­¦åˆ°å¤šæ ·æ€§

---

## 6. å‚æ•°è°ƒä¼˜

### 6.1 å¸¸ç”¨è°ƒä¼˜åœºæ™¯

#### åœºæ™¯ 1: æ˜¾å­˜ä¸è¶³

**ç—‡çŠ¶**: `RuntimeError: CUDA out of memory`

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ–¹æ¡ˆ A: å‡å° batch size + æ¢¯åº¦ç´¯ç§¯
--per_device_train_batch_size 2 \
--gradient_accumulation_steps 2 \  # ä¿æŒæœ‰æ•ˆ batch=4

# æ–¹æ¡ˆ B: å‡å° LoRA rank
--lora_rank 4 \                     # ä» 8 é™åˆ° 4

# æ–¹æ¡ˆ C: å‡å°åºåˆ—é•¿åº¦
--max_length 1024 \                 # ä» 2048 é™åˆ° 1024

# æ–¹æ¡ˆ D: ç»„åˆä½¿ç”¨
--per_device_train_batch_size 2 \
--lora_rank 4 \
--max_length 1536 \
```

#### åœºæ™¯ 2: è¿½æ±‚æ€§èƒ½

**ç›®æ ‡**: æé«˜æ¨¡å‹å®¹é‡å’Œè´¨é‡

**è§£å†³æ–¹æ¡ˆ**:
```bash
# å¢å¤§ LoRA å®¹é‡
--lora_rank 16 \                    # ä» 8 å¢å¤§åˆ° 16
--lora_alpha 32 \                   # ä¿æŒ alpha=2Ã—rank

# æ›´ç»†è‡´çš„è®­ç»ƒ
--learning_rate 5e-6 \              # é™ä½å­¦ä¹ ç‡
--num_train_epochs 3 \              # å¢åŠ è®­ç»ƒè½®æ•°
--warmup_ratio 0.1 \                # å¢åŠ  warmup

# æ›´å¤š on-policy æ¢ç´¢
--lmbda 0.7 \                       # ä» 0.5 æé«˜åˆ° 0.7
```

#### åœºæ™¯ 3: å¿«é€Ÿå®éªŒ

**ç›®æ ‡**: å¿«é€ŸéªŒè¯æƒ³æ³•

**è§£å†³æ–¹æ¡ˆ**:
```bash
# å°è§„æ¨¡å¿«é€Ÿæµ‹è¯•
--lora_rank 4 \
--per_device_train_batch_size 8 \
--dataset 'your_dataset#500' \      # åªç”¨ 500 æ¡æ•°æ®
--num_train_epochs 0.5 \            # åŠä¸ª epoch
--save_steps 10 \                   # é¢‘ç¹ä¿å­˜
--eval_steps 10 \
```

### 6.2 GKD è¶…å‚æ•°è°ƒä¼˜

#### Lambda (Î») - On-Policy é‡‡æ ·ç‡

```bash
# ä¿å®ˆï¼ˆæ›´ç¨³å®šï¼‰
--lmbda 0.3 \      # 30% on-policy, 70% off-policy

# å¹³è¡¡ï¼ˆæ¨èï¼‰
--lmbda 0.5 \      # 50% on-policy, 50% off-policy

# æ¿€è¿›ï¼ˆæ›´å¤šæ¢ç´¢ï¼‰
--lmbda 0.7 \      # 70% on-policy, 30% off-policy
```

**ç»éªŒ**:
- åˆæœŸè®­ç»ƒï¼šÎ»=0.3-0.4ï¼ˆæ›´ç¨³å®šï¼‰
- ä¸­æœŸè®­ç»ƒï¼šÎ»=0.5ï¼ˆå¹³è¡¡ï¼‰
- åæœŸè®­ç»ƒï¼šÎ»=0.6-0.7ï¼ˆæ›´å¤šè‡ªæˆ‘æ”¹è¿›ï¼‰

#### Temperature (T) - è’¸é¦æ¸©åº¦

```bash
# ä½æ¸©ï¼ˆæ¥è¿‘ç¡¬æ ‡ç­¾ï¼‰
--temperature 1.0 \

# ä¸­æ¸©ï¼ˆæ¨èï¼‰
--temperature 2.0 \

# é«˜æ¸©ï¼ˆæ›´è½¯çš„åˆ†å¸ƒï¼‰
--temperature 4.0 \
```

**ç»éªŒ**:
- å°æ¨¡å‹è’¸é¦ï¼šT=2.0-3.0
- è‡ªè’¸é¦ï¼ˆLoRAï¼‰ï¼šT=1.5-2.5
- éå¸¸å¤§çš„æ¨¡å‹ï¼šT=1.0-2.0

#### Beta (Î²) - JSD æŸå¤±æƒé‡

```bash
# æ›´é‡è§†æ•™å¸ˆ
--beta 0.3 \       # 30% å­¦ç”Ÿ, 70% æ•™å¸ˆ

# å¹³è¡¡ï¼ˆæ¨èï¼‰
--beta 0.5 \       # 50% å­¦ç”Ÿ, 50% æ•™å¸ˆ

# æ›´é‡è§†å­¦ç”Ÿ
--beta 0.7 \       # 70% å­¦ç”Ÿ, 30% æ•™å¸ˆ
```

### 6.3 LoRA é…ç½®è°ƒä¼˜

#### Rank - LoRA ç§©

```bash
# å° rankï¼ˆèŠ‚çœæ˜¾å­˜ï¼Œå¿«é€Ÿï¼‰
--lora_rank 4 \
--lora_alpha 8 \

# ä¸­ rankï¼ˆæ¨èï¼‰
--lora_rank 8 \
--lora_alpha 16 \

# å¤§ rankï¼ˆæ›´å¤§å®¹é‡ï¼‰
--lora_rank 16 \
--lora_alpha 32 \

# è¶…å¤§ rankï¼ˆè¿½æ±‚æè‡´æ€§èƒ½ï¼‰
--lora_rank 64 \
--lora_alpha 128 \
```

**å‚æ•°é‡å¯¹æ¯”**ï¼ˆd=4096ï¼‰:
- rank=4: æ¯å±‚ ~33K å‚æ•°
- rank=8: æ¯å±‚ ~66K å‚æ•°
- rank=16: æ¯å±‚ ~131K å‚æ•°
- rank=64: æ¯å±‚ ~524K å‚æ•°

#### Target Modules - åº”ç”¨èŒƒå›´

```bash
# æ‰€æœ‰çº¿æ€§å±‚ï¼ˆæ¨èï¼Œå…¨é¢ï¼‰
--lora_target_modules ALL \

# æˆ–æ˜ç¡®æŒ‡å®š
--target_modules all-linear \

# åªåº”ç”¨åˆ°æ³¨æ„åŠ›å±‚ï¼ˆèŠ‚çœï¼‰
--lora_target_modules q_proj,k_proj,v_proj,o_proj \

# åªåº”ç”¨åˆ° MLPï¼ˆå°è¯•ï¼‰
--lora_target_modules gate_proj,up_proj,down_proj \
```

---

## 7. æ˜¾å­˜ç®¡ç†

### 7.1 æ˜¾å­˜å ç”¨ä¼°ç®—

#### Qwen3-Omni-30B (8Ã—GPU, ZeRO-3)

| é…ç½® | æ¯å¡æ˜¾å­˜ | æ€»æ˜¾å­˜ | è¯´æ˜ |
|------|---------|--------|------|
| **åŸç‰ˆ GKDï¼ˆä¸¤ä¸ªç‹¬ç«‹æ¨¡å‹ï¼‰** |
| batch=4, rank=8 | ~60 GB | ~480 GB | éœ€è¦æ¨¡å‹å¹¶è¡Œ âŒ |
| **LoRA è‡ªè’¸é¦ï¼ˆå…±äº«æ¨¡å‹ï¼‰** |
| batch=2, rank=4 | ~24 GB | ~192 GB | æ˜¾å­˜ç´§å¼ æ—¶ |
| batch=2, rank=8 | ~28 GB | ~224 GB | ä¿å®ˆé…ç½® |
| batch=4, rank=8 | ~36 GB | ~288 GB | **æ¨è** âœ… |
| batch=6, rank=8 | ~45 GB | ~360 GB | æ˜¾å­˜å……è¶³æ—¶ |
| batch=4, rank=16 | ~38 GB | ~304 GB | æ›´å¤§å®¹é‡ |
| batch=8, rank=4 | ~42 GB | ~336 GB | å¤§ batch |

**èŠ‚çœ**: ~40-50% æ˜¾å­˜

### 7.2 æ˜¾å­˜ä¼˜åŒ–æŠ€å·§

#### æŠ€å·§ 1: æ¢¯åº¦ç´¯ç§¯

```bash
# åŸé…ç½®ï¼ˆæ˜¾å­˜éœ€æ±‚é«˜ï¼‰
--per_device_train_batch_size 8

# ä¼˜åŒ–åï¼ˆç›¸åŒæœ‰æ•ˆ batch sizeï¼‰
--per_device_train_batch_size 2 \
--gradient_accumulation_steps 4    # æœ‰æ•ˆ batch = 2Ã—4 = 8
```

#### æŠ€å·§ 2: æ··åˆç²¾åº¦

```bash
# BF16ï¼ˆæ¨èï¼ŒQwen3 åŸç”Ÿæ”¯æŒï¼‰
--torch_dtype bfloat16

# FP16ï¼ˆå¦‚æœGPUä¸æ”¯æŒBF16ï¼‰
--torch_dtype float16 \
--fp16 true
```

#### æŠ€å·§ 3: Flash Attention

```bash
# å¯ç”¨ Flash Attention 2
--attn_impl flash_attention_2

# å¦‚æœä¸æ”¯æŒï¼Œä½¿ç”¨æ ‡å‡†å®ç°
--attn_impl eager
```

#### æŠ€å·§ 4: DeepSpeed ä¼˜åŒ–

```bash
# ZeRO-2ï¼ˆæ›´å¿«ï¼Œä½†æ˜¾å­˜å ç”¨æ›´å¤šï¼‰
--deepspeed zero2

# ZeRO-3ï¼ˆæ¨èï¼Œæ˜¾å­˜ä¼˜åŒ–ï¼‰
--deepspeed zero3

# ZeRO-3 + Offloadï¼ˆCPUå¸è½½ï¼Œæœ€çœæ˜¾å­˜ä½†æ…¢ï¼‰
--deepspeed zero3_offload
```

### 7.3 æ˜¾å­˜ç›‘æ§

```bash
# å®æ—¶ç›‘æ§ GPU ä½¿ç”¨
watch -n 1 nvidia-smi

# æ›´è¯¦ç»†çš„ä¿¡æ¯
nvidia-smi dmon -s pucvmet

# è®­ç»ƒæ—¶è®°å½•æ˜¾å­˜å³°å€¼
nvidia-smi --query-gpu=memory.used --format=csv -l 1 > gpu_memory.log
```

**é¢„æœŸæ˜¾å­˜å ç”¨**ï¼ˆbatch=4, rank=8, ZeRO-3ï¼‰:
```
GPU 0: 36 GiB / 80 GiB
GPU 1: 36 GiB / 80 GiB
GPU 2: 36 GiB / 80 GiB
GPU 3: 36 GiB / 80 GiB
GPU 4: 36 GiB / 80 GiB
GPU 5: 36 GiB / 80 GiB
GPU 6: 36 GiB / 80 GiB
GPU 7: 36 GiB / 80 GiB
```

---

## 8. æ•…éšœæ’æŸ¥

### 8.1 OOM (æ˜¾å­˜ä¸è¶³)

**ç—‡çŠ¶**: 
```
RuntimeError: CUDA out of memory. Tried to allocate XX GiB
```

**è§£å†³æ–¹æ¡ˆ**:
1. **å‡å° batch size**
   ```bash
   --per_device_train_batch_size 2
   --gradient_accumulation_steps 2
   ```

2. **å‡å° LoRA rank**
   ```bash
   --lora_rank 4
   ```

3. **å‡å°åºåˆ—é•¿åº¦**
   ```bash
   --max_length 1024
   --max_completion_length 256
   ```

4. **å¯ç”¨ CPU Offload**
   ```bash
   --deepspeed zero3_offload
   ```

5. **å‡å°‘ workers**
   ```bash
   --dataloader_num_workers 1
   ```

### 8.2 Loss NaN æˆ–ä¸ç¨³å®š

**ç—‡çŠ¶**:
```
Step 100: loss=2.34
Step 101: loss=nan
```

**åŸå› **:
- å­¦ä¹ ç‡è¿‡é«˜
- æ¢¯åº¦çˆ†ç‚¸
- æ•°å€¼ä¸ç¨³å®š

**è§£å†³æ–¹æ¡ˆ**:
1. **é™ä½å­¦ä¹ ç‡**
   ```bash
   --learning_rate 5e-6  # æˆ– 1e-6
   ```

2. **å¢åŠ æ¢¯åº¦è£å‰ª**
   ```bash
   --max_grad_norm 1.0  # æˆ– 0.5
   ```

3. **é™ä½ lambda**
   ```bash
   --lmbda 0.3  # å‡å°‘ on-policyï¼ˆæ›´ç¨³å®šï¼‰
   ```

4. **å¢åŠ  warmup**
   ```bash
   --warmup_ratio 0.1  # æˆ– 0.2
   ```

5. **ä½¿ç”¨ BF16 ä»£æ›¿ FP16**
   ```bash
   --torch_dtype bfloat16
   ```

### 8.3 Loss ä¸ä¸‹é™

**ç—‡çŠ¶**:
```
Step 0:   loss=2.456
Step 100: loss=2.451
Step 200: loss=2.448
```

**åŸå› **:
- å­¦ä¹ ç‡è¿‡ä½
- LoRA rank å¤ªå°
- é…ç½®ä¸å½“

**è§£å†³æ–¹æ¡ˆ**:
1. **æé«˜å­¦ä¹ ç‡**
   ```bash
   --learning_rate 5e-5  # ä» 1e-5 æé«˜
   ```

2. **å¢å¤§ LoRA rank**
   ```bash
   --lora_rank 16
   --lora_alpha 32
   ```

3. **è°ƒæ•´æ¸©åº¦**
   ```bash
   --temperature 1.5  # é™ä½æ¸©åº¦
   ```

4. **æ£€æŸ¥ LoRA æ˜¯å¦ç”Ÿæ•ˆ**
   ```bash
   # æ—¥å¿—ä¸­åº”è¯¥çœ‹åˆ°
   trainable params: XXX (0.5% of total)
   ```

### 8.4 è®­ç»ƒé€Ÿåº¦æ…¢

**ç—‡çŠ¶**: it/s å¾ˆä½ï¼ˆä¾‹å¦‚ <0.5 it/sï¼‰

**è§£å†³æ–¹æ¡ˆ**:
1. **å¢åŠ  dataloader workers**
   ```bash
   --dataloader_num_workers 8
   --dataset_num_proc 8
   ```

2. **å‡å° batch sizeï¼Œå¢åŠ æ¢¯åº¦ç´¯ç§¯**
   ```bash
   --per_device_train_batch_size 2
   --gradient_accumulation_steps 2
   ```

3. **å‡å°‘ on-policy ç”Ÿæˆ**
   ```bash
   --lmbda 0.3  # on-policy ç”Ÿæˆè¾ƒæ…¢
   ```

4. **ä½¿ç”¨æ›´å¿«çš„ attention**
   ```bash
   --attn_impl flash_attention_2
   ```

### 8.5 LoRA æœªç”Ÿæ•ˆé”™è¯¯

**ç—‡çŠ¶**:
```
ValueError: GKDTrainer requires the model to have LoRA adapters.
```

**è§£å†³æ–¹æ¡ˆ**:
1. **ç¡®è®¤ LoRA å‚æ•°å­˜åœ¨**
   ```bash
   --train_type lora \
   # æˆ–
   --sft_type lora \
   --target_modules all-linear \
   ```

2. **æ£€æŸ¥ Swift ç‰ˆæœ¬**
   ```bash
   pip show ms-swift
   # ç¡®ä¿ä½¿ç”¨æœ€æ–°ç‰ˆæœ¬
   ```

3. **æ‰‹åŠ¨éªŒè¯ LoRA**
   ```python
   from peft import get_peft_model_state_dict
   # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æœ‰ LoRA å±‚
   ```

---

## 9. é«˜çº§æŠ€å·§

### 9.1 åŠ¨æ€è°ƒæ•´ Lambda

åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€æ­¥å¢åŠ  on-policy æ¯”ä¾‹ï¼š

```bash
# é˜¶æ®µ 1: ç¨³å®šæ”¶æ•›ï¼ˆ0-500 stepsï¼‰
--lmbda 0.3

# é˜¶æ®µ 2: å¹³è¡¡å­¦ä¹ ï¼ˆ500-1000 stepsï¼‰
--lmbda 0.5

# é˜¶æ®µ 3: è‡ªæˆ‘æ”¹è¿›ï¼ˆ1000+ stepsï¼‰
--lmbda 0.7
```

### 9.2 å¤šé˜¶æ®µè®­ç»ƒ

#### é˜¶æ®µ 1: å° rank å¿«é€Ÿæ”¶æ•›

```bash
# gkd_stage1.sh
--lora_rank 4 \
--lora_alpha 8 \
--num_train_epochs 1 \
--output_dir output/stage1 \
```

#### é˜¶æ®µ 2: å¤§ rank ç²¾ç»†è°ƒä¼˜

```bash
# gkd_stage2.sh
--resume_from_checkpoint output/stage1/checkpoint-XXX \
--lora_rank 16 \
--lora_alpha 32 \
--num_train_epochs 2 \
--learning_rate 5e-6 \  # é™ä½å­¦ä¹ ç‡
--output_dir output/stage2 \
```

### 9.3 æ··åˆæŸå¤±ç­–ç•¥

```bash
# æ·»åŠ  SFT æŸå¤±ï¼Œä¿è¯åŸºç¡€å‡†ç¡®æ€§
--sft_alpha 0.2 \  # 20% SFT, 80% GKD

# é€‚ç”¨åœºæ™¯ï¼š
# - æ•°æ®è´¨é‡å¾ˆé«˜
# - éœ€è¦ä¿è¯å‡†ç¡®æ€§
# - é˜²æ­¢è¿‡åº¦æ¢ç´¢
```

### 9.4 è‡ªå®šä¹‰ç”Ÿæˆé…ç½®

```bash
# æ§åˆ¶ on-policy ç”Ÿæˆçš„è´¨é‡
--temperature 1.0 \              # ç”Ÿæˆæ—¶çš„æ¸©åº¦
--top_p 0.9 \                    # Nucleus é‡‡æ ·
--top_k 50 \                     # Top-K é‡‡æ ·
--repetition_penalty 1.1 \       # é‡å¤æƒ©ç½š
```

### 9.5 å®éªŒè®°å½•

å»ºç«‹ç³»ç»Ÿçš„å®éªŒè®°å½•ä¹ æƒ¯ï¼š

```markdown
## å®éªŒ #1 - Baseline
- **æ—¥æœŸ**: 2025-11-28
- **é…ç½®**: lmbda=0.5, rank=8, batch=4, lr=1e-5
- **æ•°æ®**: alpaca-gpt4-zh (2000 samples)
- **ç¡¬ä»¶**: 8Ã—A100 80GB
- **æ˜¾å­˜**: 36 GB/GPU
- **é€Ÿåº¦**: 11s/it
- **ç»“æœ**: 
  - Final loss: 1.234
  - æ—  OOM
  - æ”¶æ•›å¹³ç¨³
- **å¤‡æ³¨**: åŸºçº¿é…ç½®ï¼Œè¿è¡Œæ­£å¸¸

## å®éªŒ #2 - Higher Rank
- **æ—¥æœŸ**: 2025-11-28
- **é…ç½®**: lmbda=0.5, rank=16, batch=4, lr=5e-6
- **ä¸ #1 çš„å·®å¼‚**: rank 8â†’16, lr è°ƒä½
- **ç»“æœ**:
  - Final loss: 1.187 (â†“ 3.8%)
  - æ˜¾å­˜: 38 GB/GPU (â†‘ 2GB)
  - é€Ÿåº¦: 13s/it (â†“ 15%)
- **ç»“è®º**: rank=16 æ€§èƒ½æå‡æ˜æ˜¾ï¼Œæ˜¾å­˜å¢åŠ å¯æ¥å—
```

---

## 10. æŠ€æœ¯æ·±å…¥

### 10.1 LoRA æ•°å­¦åŸç†

#### å‰å‘ä¼ æ’­

```
y = W_base @ x + (W_down @ W_up) @ x * (alpha / rank)

å…¶ä¸­:
  W_base: [d_out, d_in] åŸºç¡€æƒé‡ (å†»ç»“)
  W_down: [rank, d_in]  LoRA ä¸‹æŠ•å½± (å¯è®­ç»ƒ)
  W_up:   [d_out, rank] LoRA ä¸ŠæŠ•å½± (å¯è®­ç»ƒ)
  rank:   LoRA ç§© (ä¾‹å¦‚ 8)
  alpha:  ç¼©æ”¾å› å­ (ä¾‹å¦‚ 16)
```

#### å‚æ•°é‡è®¡ç®—

```
åŸºç¡€å‚æ•°: d_out Ã— d_in
LoRA å‚æ•°: rank Ã— d_in + d_out Ã— rank = rank Ã— (d_in + d_out)

ä¾‹å¦‚ d_in = d_out = 4096, rank = 8:
  åŸºç¡€: 16,777,216 å‚æ•°
  LoRA: 65,536 å‚æ•°
  æ¯”ä¾‹: 0.39%
```

### 10.2 JSD æ•°å­¦æ¨å¯¼

#### å®šä¹‰

```
JSD(P || Q) = H(M) - [Î²Â·H(P) + (1-Î²)Â·H(Q)]

å…¶ä¸­:
  M = Î²Â·P + (1-Î²)Â·Q  (æ··åˆåˆ†å¸ƒ)
  H(Â·) æ˜¯ç†µ
  Î² æ˜¯æƒé‡
```

#### ç­‰ä»·å½¢å¼ï¼ˆKL æ•£åº¦ï¼‰

```
JSD(P || Q) = Î²Â·KL(P || M) + (1-Î²)Â·KL(Q || M)

å…¶ä¸­:
  KL(P || M) = Î£ P(x) log(P(x) / M(x))
```

#### æ€§è´¨

1. **å¯¹ç§°æ€§**: JSD(P || Q) = JSD(Q || P)
2. **æœ‰ç•Œæ€§**: 0 â‰¤ JSD(P || Q) â‰¤ log(2) â‰ˆ 0.693
3. **å¹³æ»‘æ€§**: åœ¨ Pâ‰ˆQ æ—¶æ¢¯åº¦ç¨³å®š

### 10.3 æ¢¯åº¦æµå‘åˆ†æ

```
           Loss (JSD)
               â†“
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â†“                   â†“
Student Logits      Teacher Logits
     â†“                   âœ— (no_grad)
Student Model       Teacher Model
     â†“                   âœ—
åŸºç¡€å‚æ•° (å†»ç»“)     åŸºç¡€å‚æ•° (å†»ç»“)
     â†“                   âœ—
LoRA å‚æ•° âœ“         LoRA (ç¦ç”¨)
(å¯è®­ç»ƒ)             âœ—
```

**å…³é”®ç‚¹**ï¼š
1. æ•™å¸ˆæ¨¡å‹åœ¨ `torch.no_grad()` ä¸­ï¼Œ**ä¸è®¡ç®—æ¢¯åº¦**
2. å­¦ç”Ÿæ¨¡å‹çš„åŸºç¡€å‚æ•°**å†»ç»“**ï¼ˆä¸åœ¨ LoRA ä¸­ï¼‰
3. åªæœ‰ **LoRA å‚æ•°**æ¥æ”¶æ¢¯åº¦å¹¶æ›´æ–°

### 10.4 ä»£ç æ‰§è¡Œè·¯å¾„

```
1. bash gkd.sh
   â†“
2. swift rlhf --rlhf_type gkd ...
   â†“
3. swift/llm/train/rlhf.py â†’ rlhf_main()
   â†“
4. SwiftRLHF._prepare_model_tokenizer()
   - åŠ è½½ Qwen3-Omni-30B-A3B-Instruct
   - åº”ç”¨ LoRA (rank=8, target=all-linear)
   â†“
5. SwiftRLHF._get_trainer_kwargs()
   - å‡†å¤‡ teacher_model (ä¼šè¢« pop æ‰)
   â†“
6. GKDTrainer.__init__()
   - éªŒè¯æ¨¡å‹æœ‰ LoRA âœ“
   - æ‰“å° "Using shared base model architecture"
   â†“
7. Trainer.train()
   â†“
8. è®­ç»ƒå¾ªç¯å¼€å§‹
   â†“
9. GKDTrainer.training_step(model, batch)
   - å†³å®š on-policy vs off-policy
   - å¯èƒ½ç”Ÿæˆæ–°æ•°æ®
   â†“
10. GKDTrainer.compute_loss(model, inputs)
    - å­¦ç”Ÿå‰å‘ï¼ˆLoRA å¯ç”¨ï¼‰
    - æ•™å¸ˆå‰å‘ï¼ˆLoRA ç¦ç”¨ï¼‰
    - è®¡ç®— JSD loss
    â†“
11. loss.backward()
    - è®¡ç®—æ¢¯åº¦ï¼ˆåªé’ˆå¯¹ LoRA å‚æ•°ï¼‰
    â†“
12. optimizer.step()
    - æ›´æ–° LoRA æƒé‡
    â†“
13. é‡å¤æ­¥éª¤ 9-12 ç›´åˆ°è®­ç»ƒç»“æŸ
```

### 10.5 å®Œæ•´ Loss è®¡ç®—ä¼ªä»£ç 

```python
def full_training_step(model, batch, lmbda, temperature, beta):
    """å®Œæ•´çš„è®­ç»ƒæ­¥éª¤ä¼ªä»£ç """
    
    # 1. é‡‡æ ·å†³å®šæ¨¡å¼
    if random() <= lmbda:
        # On-Policy: å­¦ç”Ÿç”Ÿæˆ
        with model.eval(), lora_enabled:
            inputs = model.generate(batch)
        model.train()
    else:
        # Off-Policy: ä½¿ç”¨æ•°æ®é›†
        inputs = batch
    
    # 2. å­¦ç”Ÿå‰å‘ä¼ æ’­ï¼ˆLoRA å¯ç”¨ï¼‰
    with lora_enabled:
        logits_student = model(inputs)  # [B, L, V]
    
    # 3. æ•™å¸ˆå‰å‘ä¼ æ’­ï¼ˆLoRA ç¦ç”¨ï¼‰
    with no_grad(), lora_disabled:
        logits_teacher = model(inputs)  # [B, L, V]
    
    # 4. æ¸©åº¦è½¯åŒ–
    logits_student = logits_student / temperature
    logits_teacher = logits_teacher / temperature
    
    # 5. æå–æœ‰æ•ˆä½ç½®
    mask = (labels != -100)  # åªçœ‹ç”Ÿæˆéƒ¨åˆ†
    logits_student = logits_student[mask]  # [N, V]
    logits_teacher = logits_teacher[mask]  # [N, V]
    
    # 6. è®¡ç®—æ¦‚ç‡åˆ†å¸ƒ
    P_s = softmax(logits_student, dim=-1)
    P_t = softmax(logits_teacher, dim=-1)
    
    # 7. æ··åˆåˆ†å¸ƒ
    M = beta * P_s + (1 - beta) * P_t
    
    # 8. è®¡ç®— JSD
    kl_student = sum(P_s * log(P_s / M))
    kl_teacher = sum(P_t * log(P_t / M))
    loss = beta * kl_student + (1 - beta) * kl_teacher
    
    # 9. å½’ä¸€åŒ–
    loss = loss / N  # N = æœ‰æ•ˆ token æ•°
    
    # 10. å¯é€‰ SFT æŸå¤±
    if sft_alpha > 0:
        sft_loss = cross_entropy(logits_student, labels)
        loss = (1 - sft_alpha) * loss + sft_alpha * sft_loss
    
    # 11. åå‘ä¼ æ’­ï¼ˆåªæ›´æ–° LoRAï¼‰
    loss.backward()
    optimizer.step()
    
    return loss
```

### 10.6 å®é™…æ•°å€¼ç¤ºä¾‹

ä»¥æ‚¨çš„é…ç½®ä¸ºä¾‹ï¼š

```bash
æ¨¡å‹: Qwen3-Omni-30B-A3B-Instruct
batch_size: 4
seq_len: 2048
vocab_size: 151936
lmbda: 0.5
temperature: 2.0
beta: 0.5
```

**æ¯ä¸ªè®­ç»ƒæ­¥éª¤**ï¼š

1. **50% æ¦‚ç‡**: å­¦ç”Ÿç”Ÿæˆæ–°åºåˆ—
2. **å­¦ç”Ÿå‰å‘**: è®¡ç®— logits [4, 2048, 151936]
3. **æ•™å¸ˆå‰å‘**: è®¡ç®— logits [4, 2048, 151936]ï¼ˆLoRA ç¦ç”¨ï¼‰
4. **æå–æœ‰æ•ˆ**: å‡è®¾ 512 ä¸ªç”Ÿæˆ token
5. **è®¡ç®— JSD**: å¯¹ [512, 151936] çš„åˆ†å¸ƒ
6. **å½’ä¸€åŒ–**: loss /= 512
7. **åå‘ä¼ æ’­**: æ›´æ–° ~150M LoRA å‚æ•°
8. **æ˜¾å­˜å ç”¨**: ~36 GB/GPU

### 10.7 Loss æ›²çº¿åˆ†æ

```
Loss æ›²çº¿ (å…¸å‹æ¨¡å¼):

3.0 â”¤
    â”‚  â—
    â”‚   â—
2.5 â”¤    â—â—
    â”‚      â—â—
    â”‚        â—â—
2.0 â”¤          â—â—â—
    â”‚             â—â—â—
    â”‚                â—â—â—â—
1.5 â”¤                    â—â—â—â—â—
    â”‚                         â—â—â—â—â—â—â—
    â”‚                               â—â—â—â—â—â—â—â—
1.0 â”¤                                      â—â—â—â—â—â—â—â—â€•â€•â€•
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’
     0  100 200 300 400 500 600 700 800 900 1000  Steps

é˜¶æ®µåˆ†æ:
- åˆæœŸ (0-200):   å¿«é€Ÿä¸‹é™ (å­¦ä¹ åŸºæœ¬æ¨¡å¼)
- ä¸­æœŸ (200-600): ç¨³å®šä¸‹é™ (ç²¾ç»†è°ƒæ•´)
- åæœŸ (600+):    æ”¶æ•›å¹³ç¨³ (æ¥è¿‘æ•™å¸ˆ)
```

### 10.8 å…¬å¼æ€»ç»“è¡¨

| æ¦‚å¿µ | å…¬å¼ | è¯´æ˜ |
|------|------|------|
| **LoRA è¾“å‡º** | `y = Wx + Î”Wx` | Î”æ˜¯LoRAå¢é‡ |
| **Î”W å®šä¹‰** | `Î”W = (Î±/r)Â·W_downÂ·W_up` | Î±=alpha, r=rank |
| **JSD Loss** | `Î²Â·KL(Pâ€–M) + (1-Î²)Â·KL(Qâ€–M)` | Mæ˜¯æ··åˆåˆ†å¸ƒ |
| **æ··åˆåˆ†å¸ƒ** | `M = Î²Â·P + (1-Î²)Â·Q` | Î²é€šå¸¸=0.5 |
| **æ¸©åº¦è½¯åŒ–** | `P = softmax(z/T)` | Tè¶Šå¤§è¶Šå¹³æ»‘ |
| **KL æ•£åº¦** | `Î£ P(x)log(P(x)/Q(x))` | åˆ†å¸ƒé—´è·ç¦» |
| **On-Policyæ¦‚ç‡** | `random() â‰¤ Î»` | Î»=0.5 â†’ 50%æ¦‚ç‡ |
| **æ¢¯åº¦æ›´æ–°** | `W â† W - Î·âˆ‡L` | åªæ›´æ–°LoRA |

---

## æ€»ç»“

### âœ… æ‚¨çš„é…ç½®å·²ç»æ­£ç¡®ï¼

å½“å‰é…ç½®**å·²ç»æ˜¯ LoRA è‡ªè’¸é¦æ¨¡å¼**ï¼š
- âœ… æ•™å¸ˆå’Œå­¦ç”Ÿå…±äº«åŒä¸€ä¸ªåŸºç¡€æ¨¡å‹
- âœ… é€šè¿‡ LoRA å¯ç”¨/ç¦ç”¨æ¥åŒºåˆ†
- âœ… æ˜¾è‘—èŠ‚çœæ˜¾å­˜å’Œè®­ç»ƒæ—¶é—´ï¼ˆ~40-50%ï¼‰
- âœ… é€‚åˆ Qwen3-Omni-30B è¿™æ ·çš„å¤§å‹ MoE æ¨¡å‹

### ğŸš€ ç«‹å³å¼€å§‹

```bash
cd /Users/duduke/code/ms-swift/osum_v2
bash gkd_optimized.sh
```

### ğŸ“Š é¢„æœŸæ•ˆæœ

| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| æ˜¾å­˜èŠ‚çœ | ~40-50% |
| è®­ç»ƒåŠ é€Ÿ | ~20-30% |
| å¯è®­ç»ƒå‚æ•° | ~0.5% (LoRA only) |
| æ¯å¡æ˜¾å­˜ | ~36 GB (batch=4, rank=8) |

### ğŸ¯ ä¼˜åŒ–å»ºè®®

1. **æ˜¾å­˜å……è¶³æ—¶**: batch=6, rank=16
2. **æ˜¾å­˜ç´§å¼ æ—¶**: batch=2, rank=4, gradient_accumulation=2
3. **è¿½æ±‚æ€§èƒ½æ—¶**: rank=16, lr=5e-6, epochs=3
4. **å¿«é€Ÿå®éªŒæ—¶**: å°æ•°æ®é›†, rank=4, half epoch

### ğŸ’¡ æ ¸å¿ƒè¦ç‚¹

- Lambda(Î»): æ§åˆ¶æ¢ç´¢ç¨‹åº¦ï¼ˆ0.3ç¨³å®šï¼Œ0.7æ¢ç´¢ï¼‰
- Temperature(T): æ§åˆ¶åˆ†å¸ƒè½¯åŒ–ï¼ˆ1.0ç¡¬ï¼Œ4.0è½¯ï¼‰
- Beta(Î²): æ§åˆ¶å­¦ç”Ÿ/æ•™å¸ˆæƒé‡ï¼ˆé€šå¸¸0.5ï¼‰
- LoRA Rank: æ§åˆ¶å®¹é‡ï¼ˆ4-16å¸¸ç”¨ï¼‰

### ğŸ“ é‡åˆ°é—®é¢˜æ—¶

1. **OOM** â†’ å‡å° batch/rank/length
2. **Loss NaN** â†’ é™ä½ lr, å‡å° lambda
3. **ä¸æ”¶æ•›** â†’ å¢å¤§ lr/rank, è°ƒæ•´ temperature
4. **é€Ÿåº¦æ…¢** â†’ å¢åŠ  workers, å‡å° lambda

---

**ç¥è®­ç»ƒé¡ºåˆ©ï¼** ğŸ‰

å¦‚æœ‰ä»»ä½•é—®é¢˜ï¼Œå‚è€ƒæœ¬æ–‡æ¡£çš„ç›¸åº”ç« èŠ‚ã€‚
