# GKD LoRA è‡ªè’¸é¦ - å¿«é€Ÿå‚è€ƒ

## ğŸ¯ æ ¸å¿ƒæ¦‚å¿µ

```
åŒä¸€ä¸ªåŸºç¡€æ¨¡å‹ (Qwen3-Omni-30B)
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
    â”‚         â”‚
æ•™å¸ˆæ¨¡å¼    å­¦ç”Ÿæ¨¡å¼
(LoRAç¦ç”¨) (LoRAå¯ç”¨)
    â”‚         â”‚
å†»ç»“é¢„æµ‹   å¯è®­ç»ƒé¢„æµ‹
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
      JSDæŸå¤±
```

## âœ… å½“å‰é…ç½®ç¡®è®¤

æ‚¨çš„ `gkd.sh` **å·²ç»æ­£ç¡®é…ç½®**äº† LoRA è‡ªè’¸é¦ï¼

å…³é”®é…ç½®ï¼š
```bash
--model /path/to/Qwen3-Omni-30B-A3B-Instruct      # åŸºç¡€æ¨¡å‹
--teacher_model /path/to/Qwen3-Omni-30B-A3B-Instruct  # åŒä¸€ä¸ªï¼
--train_type lora                                  # å¯ç”¨ LoRA
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

```bash
# ç›´æ¥è¿è¡Œ
cd /Users/duduke/code/ms-swift/osum_v2
bash gkd.sh

# æˆ–ä½¿ç”¨ä¼˜åŒ–ç‰ˆ
bash gkd_optimized.sh
```

## ğŸ“Š å‚æ•°é€ŸæŸ¥è¡¨

| å‚æ•° | é»˜è®¤å€¼ | å»ºè®®èŒƒå›´ | è¯´æ˜ |
|------|--------|----------|------|
| **GKD å‚æ•°** |
| `lmbda` | 0.5 | 0.3-0.7 | On-policy é‡‡æ ·æ¦‚ç‡ |
| `temperature` | 2.0 | 1.0-4.0 | è’¸é¦æ¸©åº¦ï¼ˆè½¯åŒ–åˆ†å¸ƒï¼‰ |
| `beta` | 0.5 | 0.3-0.7 | JSD æŸå¤±æƒé‡ |
| `seq_kd` | false | true/false | åºåˆ—çº§ vs Tokençº§ KD |
| **LoRA å‚æ•°** |
| `lora_rank` | 8 | 4-64 | LoRA ç§©ï¼ˆå®¹é‡ï¼‰ |
| `lora_alpha` | 16 | 8-128 | LoRA alpha (é€šå¸¸=2Ã—rank) |
| `lora_dropout` | 0.05 | 0.0-0.1 | Dropout ç‡ |
| **è®­ç»ƒå‚æ•°** |
| `batch_size` | 4 | 1-8 | æ¯å¡ batch size |
| `learning_rate` | 1e-5 | 5e-6 ~ 5e-5 | å­¦ä¹ ç‡ |
| `grad_accum` | 1 | 1-8 | æ¢¯åº¦ç´¯ç§¯æ­¥æ•° |

## ğŸ›ï¸ å¸¸ç”¨è°ƒä¼˜åœºæ™¯

### åœºæ™¯ 1: æ˜¾å­˜ä¸è¶³
```bash
--per_device_train_batch_size 2 \
--gradient_accumulation_steps 2 \  # ä¿æŒæœ‰æ•ˆ batch=4
--lora_rank 4 \                     # å‡å° LoRA rank
```

### åœºæ™¯ 2: è¿½æ±‚æ€§èƒ½
```bash
--per_device_train_batch_size 4 \
--lora_rank 16 \                    # å¢å¤§ LoRA rank
--lora_alpha 32 \
--learning_rate 5e-6 \              # é™ä½å­¦ä¹ ç‡
```

### åœºæ™¯ 3: å¿«é€Ÿå®éªŒ
```bash
--per_device_train_batch_size 8 \
--lora_rank 4 \
--dataset 'your_dataset#500' \      # å‡å°‘æ•°æ®é‡
--num_train_epochs 0.5 \
```

## ğŸ” è¿è¡Œæ£€æŸ¥

è®­ç»ƒå¼€å§‹æ—¶ï¼Œæ—¥å¿—åº”è¯¥æ˜¾ç¤ºï¼š

```
âœ“ Using shared base model architecture:
  - Student model: base_model + LoRA (trainable)
  - Teacher model: base_model with LoRA disabled (frozen)
```

å¦‚æœçœ‹åˆ°é”™è¯¯ï¼š
```
âœ— ValueError: GKDTrainer requires the model to have LoRA adapters.
```
æ£€æŸ¥ `--train_type lora` æ˜¯å¦å­˜åœ¨ã€‚

## ğŸ’¾ æ˜¾å­˜å ç”¨ä¼°ç®—

### Qwen3-Omni-30B-A3B (8x GPU, ZeRO-3)

| é…ç½® | æ¯å¡æ˜¾å­˜ | æ€»æ˜¾å­˜ | è¯´æ˜ |
|------|---------|--------|------|
| **åŸç‰ˆ GKD** (ä¸¤ä¸ªç‹¬ç«‹æ¨¡å‹) |
| batch=4, rank=8 | ~60 GB | ~480 GB | éœ€è¦æ¨¡å‹å¹¶è¡Œ |
| **LoRA è‡ªè’¸é¦** (å…±äº«æ¨¡å‹) |
| batch=4, rank=8 | ~36 GB | ~288 GB | âœ… æ¨è |
| batch=6, rank=8 | ~45 GB | ~360 GB | æ˜¾å­˜å……è¶³æ—¶ |
| batch=2, rank=8 | ~28 GB | ~224 GB | æ˜¾å­˜ç´§å¼ æ—¶ |
| batch=4, rank=16 | ~38 GB | ~304 GB | æ›´å¤§å®¹é‡ |

**èŠ‚çœ**: ~40-50% æ˜¾å­˜

## ğŸ”§ æ•…éšœæ’æŸ¥

### é—®é¢˜ 1: OOM (æ˜¾å­˜ä¸è¶³)

**ç—‡çŠ¶**: `RuntimeError: CUDA out of memory`

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ–¹æ¡ˆ A: å‡å° batch size
--per_device_train_batch_size 2

# æ–¹æ¡ˆ B: å‡å° LoRA rank
--lora_rank 4

# æ–¹æ¡ˆ C: å‡å°åºåˆ—é•¿åº¦
--max_length 1024
```

### é—®é¢˜ 2: è®­ç»ƒä¸ç¨³å®š/Loss NaN

**ç—‡çŠ¶**: Loss çªç„¶å˜æˆ NaN æˆ–å‰§çƒˆéœ‡è¡

**è§£å†³æ–¹æ¡ˆ**:
```bash
# é™ä½å­¦ä¹ ç‡
--learning_rate 5e-6  # æˆ– 1e-6

# é™ä½ lambda (å‡å°‘ on-policy)
--lmbda 0.3

# å¢åŠ  warmup
--warmup_ratio 0.1
```

### é—®é¢˜ 3: è®­ç»ƒé€Ÿåº¦æ…¢

**ç—‡çŠ¶**: it/s å¾ˆä½

**è§£å†³æ–¹æ¡ˆ**:
```bash
# å¢åŠ  dataloader workers
--dataloader_num_workers 8

# å‡å° batch sizeï¼Œå¢åŠ æ¢¯åº¦ç´¯ç§¯
--per_device_train_batch_size 2
--gradient_accumulation_steps 2
```

## ğŸ“ˆ ç›‘æ§å‘½ä»¤

```bash
# ç›‘æ§ GPU ä½¿ç”¨
watch -n 1 nvidia-smi

# ç›‘æ§è®­ç»ƒæ—¥å¿—
tail -f output/runs/xxx/logs/xxx.log

# æŸ¥çœ‹ TensorBoard
tensorboard --logdir output/runs
```

## ğŸ¨ é«˜çº§æŠ€å·§

### æŠ€å·§ 1: åŠ¨æ€è°ƒæ•´ Lambda

```python
# å¯ä»¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€æ­¥å¢åŠ  on-policy æ¯”ä¾‹
# åˆæœŸ: lmbda=0.3 (æ›´ç¨³å®š)
# åæœŸ: lmbda=0.7 (æ›´å¤šæ¢ç´¢)
```

### æŠ€å·§ 2: å¤šé˜¶æ®µè®­ç»ƒ

```bash
# é˜¶æ®µ 1: å° rank å¿«é€Ÿæ”¶æ•›
bash gkd_stage1.sh  # rank=4, epochs=1

# é˜¶æ®µ 2: å¤§ rank ç²¾ç»†è°ƒä¼˜
# ä»é˜¶æ®µ 1 checkpoint ç»§ç»­
bash gkd_stage2.sh  # rank=16, epochs=2
```

### æŠ€å·§ 3: æ··åˆç²¾åº¦ä¼˜åŒ–

```bash
# BF16 (æ¨èï¼ŒQwen3 åŸç”Ÿæ”¯æŒ)
--torch_dtype bfloat16

# FP16 (å¦‚æœ GPU ä¸æ”¯æŒ BF16)
--torch_dtype float16
--fp16 true
```

## ğŸ“ å®éªŒè®°å½•æ¨¡æ¿

```markdown
## å®éªŒ #1
- æ—¥æœŸ: 2025-11-28
- é…ç½®: lmbda=0.5, rank=8, batch=4
- æ•°æ®: alpaca-gpt4-zh (2000 samples)
- æ˜¾å­˜: 36 GB/GPU
- é€Ÿåº¦: 11s/it
- ç»“æœ: Lossä¸‹é™å¹³ç¨³ï¼Œæ—  OOM
- å¤‡æ³¨: åŸºçº¿é…ç½®ï¼Œè¿è¡Œæ­£å¸¸
```

## ğŸ”— ç›¸å…³æ–‡ä»¶

- `gkd.sh` - æ‚¨çš„åŸå§‹è„šæœ¬
- `gkd_optimized.sh` - ä¼˜åŒ–ç‰ˆè„šæœ¬ï¼ˆæ·»åŠ äº†è¯¦ç»†æ³¨é‡Šï¼‰
- `README.md` - å®Œæ•´æ–‡æ¡£
- `swift/trainers/rlhf_trainer/gkd_trainer.py` - ä¿®æ”¹åçš„è®­ç»ƒå™¨

## ğŸ’¡ æ ¸å¿ƒè¦ç‚¹

1. âœ… **é…ç½®å·²æ­£ç¡®**: æ‚¨çš„ gkd.sh å·²ç»æ˜¯ LoRA è‡ªè’¸é¦æ¨¡å¼
2. âœ… **æ— éœ€ä¿®æ”¹**: `--teacher_model` æŒ‡å‘åŒä¸€æ¨¡å‹å³å¯
3. âœ… **è‡ªåŠ¨æ£€æµ‹**: trainer ä¼šè‡ªåŠ¨ä½¿ç”¨å…±äº«åŸºç¡€æ¨¡å‹
4. âœ… **æ˜¾å­˜å‹å¥½**: èŠ‚çœ 40-50% æ˜¾å­˜
5. âœ… **è®­ç»ƒåŠ é€Ÿ**: LoRA æ¢¯åº¦è®¡ç®—æ›´å¿«

## âš¡ ä¸€é”®å¯åŠ¨

```bash
cd /Users/duduke/code/ms-swift/osum_v2 && bash gkd.sh
```

è®­ç»ƒæ„‰å¿«ï¼ğŸš€
